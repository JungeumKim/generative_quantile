"""
TITLE
Module for training and generating data from conditional and joint distributions
using WGANs.

Author: Jonas Metzger and Evan Munro
Revise: Yuexi Wang
TITLE
"""

import torch
import math
import torch.nn as nn
import torch.nn.functional as F
from torch.utils import data as D
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from time import time
import pickle

class DataWrapper(object):
    """Class for processing raw training data for training Wasserstein GAN

    Parameters
    ----------
    df: pandas.DataFrame
        Training data frame, includes both variables to be generated, and
        variables to be conditioned on
    continuous_vars: list
        List of str of continuous variables to be generated
    categorical_vars: list
        List of str of categorical variables to be generated
    context_vars: list
        List of str of variables that are conditioned on for cWGAN
    continuous_lower_bounds: dict
        Key is element of continuous_vars, value is lower limit on that variable.
    continuous_upper_bounds: dict
        Key is element of continuous_vars, value is upper limit on that variable.


    ATTRIBUTES
    Attributes
    ----------
    variables: dict
        Includes lists of names of continuous, categorical and context variables
    means: list
        List of means of continuous and context variables
    stds: list
        List of float of standard deviation of continuous and context variables
    cat_dims: list
        List of dimension of each categorical variable
    cat_labels: list
        List of labels of each categorical variable
    cont_bounds: torch.tensor
        formatted lower and upper bounds of continuous variables
    ATTRIBUTES
    """
    def __init__(self, df, continuous_vars=[], categorical_vars=[], context_vars=[],
                 continuous_lower_bounds = dict(), continuous_upper_bounds = dict()):
        variables = dict(continuous=continuous_vars,
                         categorical=categorical_vars,
                         context=context_vars)
        self.variables = variables
        continuous, context = [torch.tensor(np.array(df[variables[_]])).to(torch.float) for _ in ("continuous", "context")]
        self.means = [x.mean(0, keepdim=True) for x in (continuous, context)]
        self.stds  = [x.std(0,  keepdim=True) + 1e-5 for x in (continuous, context)]
        self.cat_dims = [df[v].nunique() for v in variables["categorical"]]
        self.cat_labels = [torch.tensor(pd.get_dummies(df[v]).columns.to_numpy()).to(torch.float) for v in variables["categorical"]]
        self.cont_bounds = [[continuous_lower_bounds[v] if v in continuous_lower_bounds.keys() else -1e8 for v in variables["continuous"]],
                            [continuous_upper_bounds[v] if v in continuous_upper_bounds.keys() else 1e8 for v in variables["continuous"]]]
        self.cont_bounds = (torch.tensor(self.cont_bounds).to(torch.float) - self.means[0]) / self.stds[0]

    def preprocess(self, df):
        """
        Scale training data for training in WGANs

        Parameters
        ----------
        df: pandas.DataFrame
            raw training data
        Returns
        -------
        x: torch.tensor
            training data to be generated by WGAN

        context: torch.tensor
            training data to be conditioned on by WGAN
        """
        x, context = [torch.tensor(np.array(df[self.variables[_]])).to(torch.float) for _ in ("continuous", "context")]
        x, context = [(x-m)/s for x,m,s in zip([x, context], self.means, self.stds)]
        if len(self.variables["categorical"]) > 0:
            categorical = torch.tensor(pd.get_dummies(df[self.variables["categorical"]], columns=self.variables["categorical"]).to_numpy())
            x = torch.cat([x, categorical.to(torch.float)], -1)
        total = torch.cat([x, context], -1)
        if not torch.all(total==total):
            raise RuntimeError("It looks like there are NaNs your data, at least after preprocessing. This is currently not supported!")
        return x, context

    def deprocess(self, x, context):
        """
        Unscale tensors from WGAN output to original scale

        Parameters
        ----------
        x: torch.tensor
            Generated data
        context: torch.tensor
            Data conditioned on
        Returns
        -------
        df: pandas.DataFrame
            DataFrame with data converted back to original scale
        """
        continuous, categorical = x.split((self.means[0].size(-1), sum(self.cat_dims)), -1)
        continuous, context = [x*s+m for x,m,s in zip([continuous, context], self.means, self.stds)]
        if categorical.size(-1) > 0:
            categorical = torch.cat([l[torch.multinomial(p, 1)] for p, l in zip(categorical.split(self.cat_dims, -1), self.cat_labels)], -1)
        df = pd.DataFrame(dict(zip(self.variables["continuous"] + self.variables["categorical"] + self.variables["context"],
                                   torch.cat([continuous, categorical, context], -1).detach().t())))
        return df

    def apply_generator(self, generator, df):
        """
        Replaces columns in DataFrame that are generated by the generator, of
        size equal to the number of rows in the DataFrame that is passed

        Parameters
        ----------
        df: pandas.DataFrame
            Must contain columns generated by the generator,
            listed in self.variables["continuous"] and
            self.variables["categorical"]
        generator: wgan_model.Generator
            Trained generator for simulating data
        Returns
        -------
        pandas.DataFrame
            Original DataFrame with columns replaced by generated data where possible.
        """
        # replaces columns in df with data from generator wherever possible
        generator.to("cpu")
        original_columns = df.columns
        x, context = self.preprocess(df)
        x_hat = generator(context)
        df_hat = self.deprocess(x_hat, context)
        updated = self.variables["continuous"] + self.variables["categorical"]
        not_updated = [col for col in list(df_hat.columns) if col not in updated]
        df_hat = df_hat.drop(not_updated, axis=1).reset_index(drop=True)
        df = df.drop(updated, axis=1).reset_index(drop=True)
        return df_hat.join(df)[original_columns]

    def apply_critic(self, critic, df, colname="critic"):
        """
        Adds column with critic output for each row the provided Dataframe

        Parameters
        ----------
        critic: wgan_model.Critic
        df: pandas.DataFrame
        colname: str
            Name of column to add to df with critic output value
        Returns
        -------
        pandas.DataFrame
        """
        critic.to("cpu")
        x, context = self.preprocess(df)
        c = critic(x, context).detach()
        if colname in list(df.columns): df = df.drop(colname, axis=1)
        df.insert(0, colname, c[:, 0].numpy())
        return df


    def apply_discriminator(self, discriminator, df, colname="discriminator"):
        """
        Adds column with discriminator output for each row the provided Dataframe

        Parameters
        ----------
        discriminator: wgan_model.Discriminator
        df: pandas.DataFrame
        colname: str
            Name of column to add to df with discriminator output value
        Returns
        -------
        pandas.DataFrame
        """
        discriminator.to("cpu")
        x, context = self.preprocess(df)
        c = discriminator(x, context).detach()
        if colname in list(df.columns): df = df.drop(colname, axis=1)
        df.insert(0, colname, c[:, 0].numpy())
        return df

class Specifications(object):
    """Class used to set up WGAN training specifications before training
    Generator and Critic.

    Parameters
    ----------
    data_wrapper: wgan_model.DataWrapper
        Object containing details on data frame to be trained
    optimizer: torch.optim.Optimizer
        The torch.optim.Optimizer object used for training the networks, per default torch.optim.Adam
    critic_d_hidden: list
        List of int, length equal to the number of hidden layers in the critic,
        giving the size of each hidden layer.
    critic_dropout: float
        Dropout parameter for critic (see Srivastava et al 2014)
    critic_steps: int
        Number of critic training steps taken for each generator training step
    critic_lr: float
        Initial learning rate for critic
    critic_gp_factor: float
        Weight on gradient penalty for critic loss function
    discriminator_d_hidden: list
        List of int, length equal to the number of hidden layers in the discriminator,
        giving the size of each hidden layer.
    discriminator_dropout: float
        Dropout parameter for discriminator
    discriminator_steps: int
        Number of discriminator training steps taken for each generator training step
    discriminator_lr: float
        Initial learning rate for discriminator
    generator_d_hidden: list
        List of int, length equal to the number of hidden layers in generator,
        giving the size of each hidden layer.
    generator_dropout: float
        Dropout parameter for generator (See Srivastava et al 2014)
    generator_lr: float
        Initial learning rate for generator
    generator_d_noise: int
        The dimension of the noise input to the generator. Default sets to the
        output dimension of the generator.
    generator_optimizer: torch.optim.Optimizer
        The torch.optim.Optimizer object used for training the generator network if different from "optimizer", per default the same
    big_Z: boolean
        Whether to save a big referecen table of Z (noise) in advance
    max_epochs: int
        The number of times to train the network on the whole dataset.
    batch_size: int
        The batch size for each training iteration.
    test_set_size: int
        Holdout test set for calculating out of sample wasserstein distance.
    load_checkpoint: str
        Filepath to existing model weights to start training from.
    save_checkpoint: str
        Filepath of folder to save model weights every save_every iterations
    save_every: int
        If save_checkpoint is not None, then how often to save checkpoint of model
        weights during training.
    print_every: int
        How often to print training status during training.
    history_path: str
        where to save the loss log
    device: str
        Either "cuda" if GPU is available or "cpu" if not

    ATTRIBUTES
    Attributes
    ----------
    settings: dict
        Contains the neural network-related settings for training
    data: dict
        Contains settings related to the data dimension and bounds
    ATTRIBUTES
    """
    def __init__(self, data_wrapper,
                 optimizer = torch.optim.Adam,
                 activation = "relu",
                 critic_d_hidden = [128,128,128],
                 critic_dropout = 0,
                 critic_steps = 15,
                 critic_lr = 1e-4,
                 critic_gp_factor = 5,
                 discriminator_d_hidden = [128,128,128],
                 discriminator_dropout = 0.1,
                 discriminator_steps = 1,
                 discriminator_lr = 1e-4,
                 generator_d_hidden = [128,128,128],
                 generator_dropout = 0.1,
                 generator_lr = 1e-4,
                 generator_d_noise = "generator_d_output",
                 generator_optimizer = "optimizer",
                 big_Z = False,
                 max_epochs = 1000,
                 batch_size = 32,
                 test_set_size = 16,
                 load_checkpoint = None,
                 save_checkpoint = None,
                 save_every = 100,
                 print_every = 200,
                 history_path = None,
                 device = "cuda" if torch.cuda.is_available() else "cpu"):

        self.settings = locals()
        del self.settings["self"], self.settings["data_wrapper"]
        d_context = len(data_wrapper.
                        variables["context"])
        d_cont = len(data_wrapper.variables["continuous"])
        d_x = d_cont + sum(data_wrapper.cat_dims)
        if generator_d_noise == "generator_d_output":
            self.settings.update(generator_d_noise = d_x)
        self.data = dict(d_context=d_context, d_x=d_x,
                         cat_dims=data_wrapper.cat_dims,
                         cont_bounds=data_wrapper.cont_bounds)

        print("settings:", self.settings)


class Generator(nn.Module):
    """
    torch.nn.Module class for generator network in WGAN

    Parameters
    ----------
    specifications: wgan_model.Specifications
        parameters for training WGAN

    ATTRIBUTES
    Attributes
    ----------
    cont_bounds: torch.tensor
        formatted lower and upper bounds of continuous variables
    cat_dims: list
        Dimension of each categorical variable
    d_cont: int
        Total dimension of continuous variables
    d_cat: int
        Total dimension of categorical variables
    d_noise: int
        Dimension of noise input to generator
    layers: torch.nn.ModuleList
        Dense neural network layers making up the generator
    dropout: torch.nn.Dropout
        Dropout layer based on specifications
    ATTRIBUTES
    """
    def __init__(self, specifications):
        super().__init__()
        s, d = specifications.settings, specifications.data
        self.cont_bounds = d["cont_bounds"]
        self.cat_dims = d["cat_dims"]
        self.d_cont = self.cont_bounds.size(-1)
        self.d_cat = sum(d["cat_dims"])
        self.d_noise = s["generator_d_noise"]
        d_in = [self.d_noise + d["d_context"]] + s["generator_d_hidden"]
        d_out = s["generator_d_hidden"] + [self.d_cont + self.d_cat]
        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(d_in, d_out)])
        self.dropout = nn.Dropout(s["generator_dropout"])
        self.activation = s["activation"]

    def _transform(self, hidden):
        continuous, categorical = hidden.split([self.d_cont, self.d_cat], -1)
        if continuous.size(-1) > 0: # apply bounds to continuous
            bounds = self.cont_bounds.to(hidden.device)
            continuous = torch.stack([continuous, bounds[0:1].expand_as(continuous)]).max(0).values
            continuous = torch.stack([continuous, bounds[1:2].expand_as(continuous)]).min(0).values
        if categorical.size(-1) > 0: # renormalize categorical
            categorical = torch.cat([F.softmax(x, -1) for x in categorical.split(self.cat_dims, -1)], -1)
        return torch.cat([continuous, categorical], -1)

    def forward(self, context, noise = None):
        """
            Run generator model

        Parameters
        ----------
        context: torch.tensor
            Variables to condition on
        noise: torch.tensor
            Latent Variable to feed

        Returns
        -------
        torch.tensor
        """
        if noise is None:
            noise = torch.randn(context.size(0), self.d_noise).to(context.device)
        x = torch.cat([noise, context], -1)
        for layer in self.layers[:-1]:
            if self.activation == "relu":
                x = self.dropout(F.relu(layer(x)))
            elif self.activation == "leaky_relu":
                x = self.dropout(F.leaky_relu(layer(x), negative_slope=0.1))
            else:
                print("error: unknown activation")
        return self._transform(self.layers[-1](x))


class Critic(nn.Module):
    """
    torch.nn.Module for critic in WGAN framework

    Parameters
    ----------
    specifications: wgan_model.Specifications

    ATTRIBUTES
    Attributes
    ----------
    layers: torch.nn.ModuleList
        Dense neural network making up the critic
    dropout: torch.nn.Dropout
        Dropout layer applied between each of hidden layers
    ATTRIBUTES
    """
    def __init__(self, specifications):
        super().__init__()
        s, d = specifications.settings, specifications.data
        d_in = [d["d_x"] + d["d_context"]] + s["critic_d_hidden"]
        d_out = s["critic_d_hidden"] + [1]
        self.layers = nn.ModuleList([nn.Linear(i, o) for i, o in zip(d_in, d_out)])
        self.dropout = nn.Dropout(s["critic_dropout"])
        self.activation = s["activation"]

    def forward(self, x, context):
        """
        Run critic model

        Parameters
        ----------
        x: torch.tensor
            Real or generated data
        context: torch.tensor
            Data conditioned on

        Returns
        -------
        torch.tensor
        """
        x = torch.cat([x, context], -1)
        for layer in self.layers[:-1]:
            if self.activation == "relu":
                x = self.dropout(F.relu(layer(x)))
            elif self.activation == "leaky_relu":
                x = self.dropout(F.leaky_relu(layer(x), negative_slope=0.1))
            else:
                print("error: unknown activation")
        return self.layers[-1](x)

    def gradient_penalty(self, x, x_hat, context):
        """
        Calculate gradient penalty

        Parameters
        ----------
        x: torch.tensor
            real data
        x_hat: torch.tensor
            generated data
        context: torch.tensor
            context data

        Returns
        -------
        torch.tensor
        """
        alpha = torch.rand(x.size(0)).unsqueeze(1).to(x.device)
        interpolated = x * alpha + x_hat * (1 - alpha)
        interpolated = torch.autograd.Variable(interpolated.detach(), requires_grad=True)
        critic = self(interpolated, context)
        gradients = torch.autograd.grad(critic, interpolated, torch.ones_like(critic),
                                        retain_graph=True, create_graph=True, only_inputs=True)[0]
        penalty = F.relu(gradients.norm(2, dim=1) - 1).mean()             # one-sided
        # penalty = (gradients.norm(2, dim=1) - 1).pow(2).mean()          # two-sided
        return penalty


def train(generator, critic, x, context, specifications, penalty=None):
    """
    Function for training generator and critic in conditional WGAN-GP
    If context is empty, trains a regular WGAN-GP. See Gulrajani et al 2017
    for details on training procedure.

    Parameters
    ----------
    generator: wgan_model.Generator
        Generator network to be trained
    critic: wgan_model.Critic
        Critic network to be trained
    x: torch.tensor
        Training data for generated data
    context: torch.tensor
        Data conditioned on for generating data
    specifications: wgan_model.Specifications
        Includes all the tuning parameters for training
    """
    # setup training objects
    s = specifications.settings
    start_epoch, step, description, device, t = 0, 1, "", s["device"], time()
    generator.to(device), critic.to(device)
    opt_generator = s["optimizer"] if s["generator_optimizer"]=="optimizer" else s["generator_optimizer"]
    opt_generator = opt_generator(generator.parameters(), lr=s["generator_lr"])
    opt_critic = s["optimizer"](critic.parameters(), lr=s["critic_lr"])
    train_batches, test_batches = D.random_split(D.TensorDataset(x, context), (x.size(0)-s["test_set_size"], s["test_set_size"]))
    train_batches, test_batches = (D.DataLoader(d, s["batch_size"], shuffle=True) for d in (train_batches, test_batches))

    # load checkpoints
    if s["load_checkpoint"]:
        if s['device']=='cpu':
            cp = torch.load(s["load_checkpoint"], map_location=torch.device('cpu'))
        else:
            cp = torch.load(s["load_checkpoint"], map_location="cuda:0")
        generator.load_state_dict(cp["generator_state_dict"])
        opt_generator.load_state_dict(cp["opt_generator_state_dict"])
        critic.load_state_dict(cp["critic_state_dict"])
        opt_critic.load_state_dict(cp["opt_critic_state_dict"])
        start_epoch, step = cp["epoch"], cp["step"]
    # start training
    try:
        for epoch in range(start_epoch, s["max_epochs"]):
            # train loop
            WD_train, n_batches = 0, 0
            for x, context in train_batches:
                x, context = x.to(device), context.to(device)
                generator_update = step % s["critic_steps"] == 0
                for par in critic.parameters():
                    par.requires_grad = not generator_update
                for par in generator.parameters():
                    par.requires_grad = generator_update
                if generator_update:
                    generator.zero_grad()
                else:
                    critic.zero_grad()
                x_hat = generator(context)
                critic_x_hat = critic(x_hat, context).mean()
                if not generator_update:
                    critic_x = critic(x, context).mean()
                    WD = critic_x - critic_x_hat
                    loss = - WD
                    loss += s["critic_gp_factor"] * critic.gradient_penalty(x, x_hat, context)
                    loss.backward()
                    opt_critic.step()
                    WD_train += WD.item()
                    n_batches += 1
                else:
                    loss = - critic_x_hat
                    if penalty is not None:
                        loss += penalty(x_hat, context)
                    loss.backward()
                    opt_generator.step()
                step += 1
            WD_train /= max(n_batches,1)
            # test loop
            WD_test, n_batches = 0, 0
            for x, context in test_batches:
                x, context = x.to(device), context.to(device)
                with torch.no_grad():
                    x_hat = generator(context)
                    critic_x_hat = critic(x_hat, context).mean()
                    critic_x = critic(x, context).mean()
                    WD_test += (critic_x - critic_x_hat).item()
                    n_batches += 1
            WD_test /= max(n_batches,1)
            # diagnostics
            if epoch % s["print_every"] == 0:
                description = "epoch {} | step {} | WD_test {} | WD_train {} | sec passed {} |".format(
                epoch, step, round(WD_test, 2), round(WD_train, 2), round(time() - t))
                print(description)
                t = time()
            if s["save_checkpoint"] and epoch % s["save_every"] == 0:
                torch.save({"epoch": epoch, "step": step,
                            "generator_state_dict": generator.state_dict(),
                            "critic_state_dict": critic.state_dict(),
                            "opt_generator_state_dict": opt_generator.state_dict(),
                            "opt_critic_state_dict": opt_critic.state_dict()}, s["save_checkpoint"])
    except KeyboardInterrupt:
        print("exited gracefully.")

def train_local_G(generator, critic, x, context, context0, specifications, penalty=None, local_G=True):
    """
    Function for training generator and critic in conditional WGAN-GP
    If context is empty, trains a regular WGAN-GP. See Gulrajani et al 2017
    for details on training procedure.

    Parameters
    ----------
    generator: wgan_model.Generator
        Generator network to be trained
    critic: wgan_model.Critic
        Critic network to be trained
    x: torch.tensor
        Training data for generated data
    context: torch.tensor
        Data conditioned on for generating data
    context0: torch.tensor
        Data conditioned on for training generator locally
    specifications: wgan_model.Specifications
        Includes all the tuning parameters for training
    local_G: logical
        Whether the generator is updated on context0 (oserved data) only
    """
    # setup training objects
    s = specifications.settings
    start_epoch, step, description, device, t = 0, 1, "", s["device"], time()
    generator.to(device), critic.to(device)
    opt_generator = s["optimizer"] if s["generator_optimizer"]=="optimizer" else s["generator_optimizer"]
    opt_generator = opt_generator(generator.parameters(), lr=s["generator_lr"])
    opt_critic = s["optimizer"](critic.parameters(), lr=s["critic_lr"])
    train_batches, test_batches = D.random_split(D.TensorDataset(x, context), (x.size(0)-s["test_set_size"], s["test_set_size"]))
    train_batches, test_batches = (D.DataLoader(d, s["batch_size"], shuffle=True) for d in (train_batches, test_batches))

    # load checkpoints
    if s["load_checkpoint"]:
        cp = torch.load(s["load_checkpoint"])
        generator.load_state_dict(cp["generator_state_dict"])
        opt_generator.load_state_dict(cp["opt_generator_state_dict"])
        critic.load_state_dict(cp["critic_state_dict"])
        opt_critic.load_state_dict(cp["opt_critic_state_dict"])
        start_epoch, step = cp["epoch"], cp["step"]
    # start training
    try:
        context0=context0[:s["batch_size"],:]
        context0 = context0.to(device)
        for epoch in range(start_epoch, s["max_epochs"]):
            # train loop
            WD_train, n_batches = 0, 0
            for x, context in train_batches:
                x, context = x.to(device), context.to(device)
                generator_update = step % s["critic_steps"] == 0
                for par in critic.parameters():
                    par.requires_grad = not generator_update
                for par in generator.parameters():
                    par.requires_grad = generator_update
                if generator_update:
                    generator.zero_grad()
                else:
                    critic.zero_grad()
                # we update the generator only locally on observed data
                if generator_update and local_G:
                    x_hat = generator(context0)
                    critic_x_hat = critic(x_hat, context0).mean()
                else:
                    x_hat = generator(context)
                    critic_x_hat = critic(x_hat, context).mean()

                if not generator_update:
                    critic_x = critic(x, context).mean()
                    WD = critic_x - critic_x_hat
                    loss = - WD
                    loss += s["critic_gp_factor"] * critic.gradient_penalty(x, x_hat, context)
                    loss.backward()
                    opt_critic.step()
                    WD_train += WD.item()
                    n_batches += 1
                else:
                    loss = - critic_x_hat
                    if penalty is not None:
                        loss += penalty(x_hat, context)
                    loss.backward()
                    opt_generator.step()
                step += 1
            WD_train /= max(n_batches,1)
            # test loop
            WD_test, n_batches = 0, 0
            for x, context in test_batches:
                x, context = x.to(device), context.to(device)
                with torch.no_grad():
                    x_hat = generator(context)
                    critic_x_hat = critic(x_hat, context).mean()
                    critic_x = critic(x, context).mean()
                    WD_test += (critic_x - critic_x_hat).item()
                    n_batches += 1
            WD_test /= n_batches
            # diagnostics
            if epoch % s["print_every"] == 0:
                description = "epoch {} | step {} | WD_test {} | WD_train {} | sec passed {} |".format(
                epoch, step, round(WD_test, 2), round(WD_train, 2), round(time() - t))
                print(description)
                t = time()
            if s["save_checkpoint"] and epoch % s["save_every"] == 0:
                torch.save({"epoch": epoch, "step": step,
                            "generator_state_dict": generator.state_dict(),
                            "critic_state_dict": critic.state_dict(),
                            "opt_generator_state_dict": opt_generator.state_dict(),
                            "opt_critic_state_dict": opt_critic.state_dict()}, s["save_checkpoint"])
    except KeyboardInterrupt:
        print("exited gracefully.")


